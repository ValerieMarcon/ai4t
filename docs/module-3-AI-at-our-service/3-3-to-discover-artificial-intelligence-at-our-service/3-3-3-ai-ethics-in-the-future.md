---
title: AI Ethics "in the future"
description:
---

### AI Ethics in the future

_Text translated from the online press article in French [(www.lemonde.fr)](https://www.lemonde.fr/blog/binaire/2020/03/25/une-ethique-de-lia-au-futur/) about ethics and machine learning._

Artificial Intelligence (AI) is at the heart of many debates and controversies in society. No area of social and economic life seems to be spared from the subject. What is interesting to note is that in all the debates today around AI, ethics is always called upon. No one seems to reduce the question of AI to a simple technical issue. This is evidenced by the proliferation of reports on the ethics of AI, produced by private companies, public actors and civil society organisations.

Among the reports, the statements may differ somewhat, but they all seem to subscribe to the same form of ethical imperative. This is the injunction to anticipate the impacts of technologies. Ethics - to use the words of the philosopher Hans Jonas - must today become "ethics of the future". Anticipating the impacts of the development of AI therefore becomes an ethical imperative.

Such an imperative is not new. It is notably at the heart of the "responsible research and innovation" approach advocated by the European Commission. It seems to me that such an analogous concept of responsibility is also at the heart of recent French reports on AI. I will focus here on three recent reports: the CNIL report <sup>1</sup>, the Villani report <sup>2</sup>, and the CERNA report <sup>3</sup>. Let's start with the Villani report. This report states that: "the law cannot do everything, among other things because the time of the law is much longer than that of the code. It is therefore essential that the "architects" of the digital society (...) take their fair share of this mission by acting responsibly. This implies that they are fully aware of the possible negative effects of their technologies on society and that they actively work to limit them.

The movement is twofold: anticipating the downstream side of technological development, in order to - upstream - modify the design to prevent negative ethical impacts. To support this approach, the Villani report envisages obliging AI developers to carry out a discrimination impact assessment in order to "force them to ask the right questions at the right time". The CNIL report also contains a similar recommendation: "Work on the design of algorithmic systems in the service of human freedom". Anticipate to integrate ethics as early as possible in the technological development process. The desire to implement "ethics in the future" is quite commendable, but many questions remain unanswered which I would like to review.

**Can the future be anticipated?**  

First of all, there is an epistemological criticism. How can we ethically judge the potentialities opened up by digital technology? Faced with such a development, are we not plunged into what economists call 'radical uncertainty'? The possibilities opened up by "Big Data" are a good example of this radical uncertainty. This term refers to the phenomenon of the uninterrupted and exponential proliferation of data. This development has led to an evolution in the technical language used to measure the storage power of data. From bytes, we have moved on to megabytes, gigabytes, etc. As Eric Sardin points out, with units of measurement such as the petabyte, the zeta-byte or the yottabyte, it is clear that we are dealing with units of measurement that purely and simply exceed our human structures of intelligibility <sup>4</sup>. Moreover, don't all the reports insist on the unpredictability of certain learning algorithms? Aware of this unpredictability, the CNIL defends in its report a principle of "vigilance" which institutes the obligation of continuous ethical reflection. But more fundamentally, should we not recognise that not everything can be anticipated?

![](https://storage.googleapis.com/prd-blogs/2020/03/c1d499a6-bytes.jpg)

Photo credit: [Ian-S](https://visualhunt.co/a4/8129d567) on [Visualhunt.com](https://visualhunt.com/re6/e9ca0ca4) / [CC BY-NC-ND](http://creativecommons.org/licenses/by-nc-nd/2.0/)

**A 'colonised' future?**

It is also important to realise that this future that we are enjoined to anticipate is saturated with fears, expectations and promises. The future is not an empty time, but a time constructed by scenarios, road-maps and prophetic discourses. To use Didier Bigo's expression, the future is 'colonised' by numerous actors who try to impose their vision as a common matrix for all anticipation of the future. Initially, he coined this expression in the context of a reflection on surveillance technologies, to designate the claims and strategies of experts who apprehend the future as a "future before, as a future already fixed, a future whose events they know" <sup>5</sup>.

The robotic ethic runs the same risk of colonising the future, as Paul Dumouchel and Luisa Damiano illustrate in their book Living with Robots. The latter are thinking in particular of authors such as Wallach and Allen, in Moral Machines, Teaching Robots Right from Wrong, who propose a programme to teach robots the difference between right and wrong, to make them "artificial moral agents". But isn't 'programming' a morally autonomous agent a contradiction in terms? I do not intend to enter into a metaphysical debate on this subject. Rather, I would like to point out that such a programme has literally 'colonised' the horizon of expectation of debates on robotic ethics. Let us follow Dumouchel and Damiano to grasp this point. They note that, in the opinion of some of the protagonists of robotic ethics, "we are still far from being able to create autonomous artificial agents that could be true moral agents. We don't even know if we will ever be able to do that," they say. We don't know if such machines are possible" <sup>6</sup>. One of the answers put forward by the proponents of robotic ethics is then that "we should not wait to elaborate such rules until we are caught unawares by the sudden irruption of autonomous artificial agents. It is important already to prepare for an inevitable future. Philosophers must already participate in the development of the robots that will populate our daily lives in the future by developing strategies to inscribe moral rules in robots that constrain their behaviour" <sup>7</sup>.

This theme of the inevitable empowerment of machines is powerful and quite problematic. Of all the possibilities, attention is focused on this scenario, which is posited as 'inevitable'. Such a focus of attention poses several problems. On the one hand, it raises epistemological questions: why is it possible to support this inevitability? On the other hand, for Dumouchel and Damiano, this belief, although not rational, has certain real effects: it diverts attention from power issues that are already at stake, namely that the empowerment of robots, the fact of delegating decisions to them and letting them choose for themselves, may mean the loss of decision-making power for a few, but it also intensifies the concentration of decision-making in the hands of a few (programmers, robot owners, etc.).

![Photo on data representation](https://storage.googleapis.com/prd-blogs/2020/03/7b8b81cc-forbiden-planet.jpg)

Image form the film  Forbiden Planet (USA, 1956) – MGM productions

**Who anticipates?**

A related question is who will do this anticipation of ethical impacts? Who will be the perpetrators of the discriminatory impacts? Is it only researchers?  On this point, all the reports make it clear that this ethical imperative to anticipate does not only concern researchers. The CERNA report specifies that "researchers must deliberate from the outset of their project with the persons or groups identified as being likely to be influenced". For the Villani report, "We must create a real forum for debate, plural and open to society, in order to determine democratically what AI we want for our society". As for the CNIL, it states in its report that "Algorithmic and artificial intelligence systems are complex socio-technical objects, shaped and manipulated by long and complex chains of actors. It is therefore all along the algorithmic chain (from the designer to the end user, through those who train the systems and those who deploy them) that action must be taken, using a combination of technical and organisational approaches. Algorithms are everywhere: they are therefore everyone's business".

The reports cited do pay attention to the question of WHO anticipates. However, the injunction to deliberate with "individuals or groups identified as potentially being influenced" (CERNA) is not enough. Determining the list of people concerned must be an object of reflection and research, not a prerequisite for this anticipation process. Indeed, anticipating these impacts can make us aware of new stakeholders to be included in the reflection. Moreover, what form should be given to an ethics of AI that is "everybody's business"? How can it be instituted?

**A consideration of time that obscures space**

Finally, a last question left unanswered is the privilege given to time over space in ethical reflection. The development of digital technology, and in particular AI, has contributed to the idea that the virtualisation of exchanges would reduce distances and the importance of places. Such a belief is, for example, at the heart of the development of telemedicine: a chronically ill person can be monitored indifferently at home or in an institution, a specialist can be called upon by tele-expertise regardless of the distance separating him from his colleague, etc. However, as the sociologists Alexandre Mathieu-Fritz and Gérald Gaglio state, "telemedicine does not lead to the abolition of borders and spaces, contrary to the common sense view often implicitly conveyed by public policies"<sup>8</sup>. In order to be effective, telemedicine requires a certain amount of spatial planning. In an ethnographic study carried out with telemonitored patients, Nelly Oudshoorn has shown the extent to which places, domestic space and public space influence and shape the way in which technologies are implemented, just as, conversely, these technologies literally transform these spaces. The home thus becomes a hybrid place, a medicalised living space. This spatial dimension does not really seem to be taken into account.

However, an author such as Pierre Rosanvallon reminded us ten years ago in his book La légitimité démocratique that the legitimacy of public action today depends more and more on what he calls a 'principle of proximity', an attention to local contexts. This is demonstrated by the promotion of an "experimentation" approach in the field of AI in health: "in order to benefit from the advances of AI in medicine, it is important to facilitate experimentations of AI technology in health in real time and as close as possible to the users, by creating the necessary regulatory and organisational conditions" (Villani Report). In a more inductive way and starting from the field, it would be a question of favouring a new construction of public action: "experimenting with public action" as Clément Bertholet and Laura Létourneau invite us to do <sup>9</sup>.

While this experimental approach invites us to take into account local contextual realities, the objective is always 'scalability' (the fact that it can be used at different scales). I take this term from the anthropologist Anna Lowenhaupt Tsing, who defines it as: "the capacity of projects to expand without changing the framework of their hypothesis" <sup>10</sup>. It is true that we experiment locally, but it seems to me that we do so in order to identify what can be generalised and made operational in an undifferentiated way at different scales. Can a remote monitoring system - to take this example - be applied in any context? Can all living spaces become places of care?

Does the ethics of AI take into account the fact that the impacts of AI will differ according to places and spaces? Where should ethical impact assessments take place? Should they be centralised or localised in the places where technical objects are designed? More fundamentally, is it possible to determine these impacts without moving? On this point, the mapping of the global landscape of AI ethics carried out by A. Jobin et al <sup>11</sup> is quite instructive. They conducted a comparative analysis of 84 reports on AI ethics. These are documents produced by government agencies, private firms, non-profit organisations and learned societies. Their analysis highlights the fact that the majority of reports are produced in the USA (20 reports), the European Union (19), followed by the UK (14) and Japan (4). African and Latin American countries are not represented independently of international or supra-national organisations. Does this geographical distribution not indicate that this spatial dimension is being overlooked?

**Alain Loute** (Centre d’éthique médicale, labo ETHICS EA 7446, Université Catholique de Lille - Centre for Medical Ethics /Catholic University of Lille)

<sup>1</sup>(https://www.lemonde.fr/#_ftnref1) [Comment permettre à l’homme de garder la main](https://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_garder_la_main_web.pdf) ? Les enjeux éthiques des algorithmes et de l’intelligence artificielle, CNIL, 2017 (How to enable man to keep his hand in, in.The ethical challenges of algorithms and artificial intelligence).

<sup>2</sup>(https://www.lemonde.fr/#_ftnref2) [Donner un sens à l’intelligence artificielle, Pour une stratégie nationale et européenne](https://www.aiforhumanity.fr/pdfs/9782111457089_Rapport_Villani_accessible.pdf), Rapport Villani, 8 mars 2018 (Giving meaning to artificial intelligence, For a national and European strategy).

<sup>3</sup>(https://www.lemonde.fr/#_ftnref3) [Ethique de la recherche en apprentissage machine](http://cerna-ethics-allistene.org/digitalAssets/53/53991_cerna___thique_apprentissage.pdf), Avis de la Commission de réflexion sur l’Ethique de la Recherche en sciences et technologies du Numérique d’Allistene (CERNA), juin 2017 (Ethics of machine learning research, in.Opinion of the Allistene Commission on Research Ethics in Digital Science and Technology).

<sup>4</sup> E. Sardin, La vie algorithmique, Critique de la raison numérique, Ed. L’échappée, Paris 2015, pp. 21-22 (Algorithmic Life, Critique of Digital Reason).

<sup>5</sup> D. Bigo, « Sécurité maximale et prévention ? La matrice du futur antérieur et ses grilles », in B. Cassin (éd.), Derrière les grilles, Sortons du tout-évaluation, Paris, Fayard, 2014, p. 111-138, p. 126 (Maximum security and prevention? The future tense matrix and its grids).

<sup>6</sup> P. Dumouchel et L. Damiano, Vivre avec des robots, Essai sur l’empathie artificielle, Paris, Seuil, 2016, p. 191 (Living with robots, An essay on artificial empathy).

<sup>7</sup> ibid., p. 191-192.

<sup>8</sup> A. Mathieu-Fritz et G. Gaglio, « À la recherche des configurations sociotechniques de la télémédecine, Revue de littérature des travaux de sciences sociales », in Réseaux, 207, 2018/1, pp. 27-63 (In search of the socio-technical configurations of telemedicine).

<sup>9</sup> C. Bertholet et L. Létourneau, Ubérisons l’Etat avant que les autres ne s’en chargent, Paris, Armand Collin, 2017, p.182 (Let's uberise the state before others do it).

<sup>10</sup> A. Lowenhaupt Tsing, Le champignon de la fin du monde, Sur la possibilité de vivre dans les ruines du capitalisme, Paris, La Découverte, 2017, p. 78(The mushroom at the end of the world, On the possibility of living in the ruins of capitalism).

<sup>11</sup> A. Jobin, M. Ienca, & E. Vayena, « The global landscape of AI ethics guidelines », in Nat Mach Intell, 1, 2019, pp. 389–399.
